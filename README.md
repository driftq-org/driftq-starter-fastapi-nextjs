# DriftQ Starter: FastAPI + Next.js (2â€‘Minute DLQ â†’ Replay â†’ Success Demo) ğŸš€

This repo is a **tiny "show it, donâ€™t explain it" demo** of DriftQ running behind a normal **FastAPI + Next.js** app.

## Architecture
![DriftQ Starter data flow](docs/flowchart.png)

What you'll see in ~2 minutes:
- FastAPI starts a "run" (basically "do this job") and publishes a command to DriftQ (think: enqueue work with a payload)
- DriftQ holds that command durably (and the event stream for the run)
- A worker (your executor) picks up that command from DriftQ. In a real app this worker is usually where youâ€™d do stuff like:
  - call an LLM (OpenAI/Anthropic/local)
  - hit external APIs (Slack/Jira/GitHub/etc)
  - run multi-step logic
- If the work fails, the worker retries automatically (up to max_attempts). If it still canâ€™t succeed, it writes a **DLQ** record with the original payload + error so the job isnâ€™t lost.
- Next.js UI watches everything live via **SSE** (Server-Sent Events). Youâ€™ll see the run timeline update in real time, inspect the DLQ payload, and then hit **Replay (fix applied)** to re-run the same job after "fixing" the cause (in this demo, replay clears fail_at so it succeeds).

> Heads up: **DriftQ itself is way more powerful** than what this repo shows.
> This is intentionally the smallest slice that proves the "DLQ â†’ Replay â†’ Success" story.
> If you want the real engine + features, check out the **[DriftQ-Core](https://github.com/driftq-org/DriftQ-Core)** repo

---

## The punchline âœ…

Open the UI, click **ğŸ—‚ï¸ 2â€‘Minute Demo**, and you should see:

1) **Fail (forced)**
2) **DLQ persisted** (inspect the payload)
3) **Replay with fix applied** (replay overrides `fail_at`)
4) **Success** ğŸ‰

If it doesnâ€™t reach Success, itâ€™s almost always one of these:
- the worker isnâ€™t running (manual path), or
- the replay endpoint isnâ€™t actually applying `{"fail_at": null}`

---

## Fail modes (what that dropdown actually means) ğŸ›ï¸

In the UI youâ€™ll see a `Fail:` dropdown. This is **demo-only** â€” itâ€™s just a chaos knob to force a failure at a specific stage so you can prove retries + DLQ + replay.

### **Fail: none**
- Meaning: **donâ€™t force any failure**
- What happens: the run should succeed normally âœ…
- Real-world vibe: "happy path" (tools/API calls returned, transforms succeeded, etc.)

### **Fail: transform**
- Meaning: force a failure during the **transform / processing step**
- What happens: the worker fails while doing internal logic â†’ retries â†’ DLQ after max attempts âŒ
- Real-world vibe: "my code messed up":
  - schema mismatch, parsing errors
  - unexpected nulls / missing fields
  - business logic throws, validation fails

### **Fail: tool_call**
- Meaning: force a failure during the **external call / tool step**
- What happens: the worker fails when it tries to call something outside itself â†’ retries â†’ DLQ âŒ
- Real-world vibe: "the outside world messed up":
  - LLM provider rate limits / timeouts / 5xx
  - external APIs failing (Slack/Jira/GitHub/etc)
  - DB / vector DB (Qdrant) issues
  - flaky network stuff

Quick mental model:
- **transform** = "my code"
- **tool_call** = "external dependency (LLM/tool/API)"
- **none** = "no forced chaos"

---

## Quickstart (recommended) ğŸ”¥

If youâ€™re sharing this repo with someone, **donâ€™t make them run 3 terminals**. Just do:

```bash
python api/scripts/dev_up.py
```

That starts everything (DriftQ + API + worker + UI) via Docker Compose.

When itâ€™s up, youâ€™ll have:
- **UI:** http://localhost:3000
- **API docs:** http://localhost:8000/docs
- **DriftQ health:** http://localhost:8080/v1/healthz

If youâ€™re running this in **GitHub Codespaces** (or any remote dev env), **donâ€™t use `localhost`**.
Use the **forwarded URL** for each port instead:
- UI: forwarded URL for port **3000**
- API docs/health: forwarded URL for port **8000**
- DriftQ health: forwarded URL for port **8080**

Then click **ğŸ—‚ï¸ 2â€‘Minute Demo** in the UI.

## Bring everything down ğŸ§¹

Normal "stop everything" (keeps volumes / WAL data):
```bash
python api/scripts/dev_down.py
```

If you want to **wipe everything** (including volumes / WAL / all persisted state):
```bash
python api/scripts/dev_down.py --wipe
```

If you want to be extra aggressive and also remove built images for this stack:
```bash
python api/scripts/dev_down.py --wipe --prune-images
```

---

## Manual local dev (3 terminals)

If you prefer running without Docker:

### 0) Prereqs
- Python **3.11+**
- Node **18+**
- DriftQ running somewhere (or use Docker for DriftQ only)

### 1) API (Terminal 1)
```bash
cd api

python -m venv .venv
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

pip install -r requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 2) Worker (Terminal 2)
```bash
cd api
# activate the same venv

python -m app.worker
```

### 3) UI (Terminal 3)
```bash
cd web
npm install

# make sure web/.env.local contains:
# NEXT_PUBLIC_API_URL=http://localhost:8000
# Codespaces: use the forwarded URL for port 8000

npm run dev
```

Open:
- UI: http://localhost:3000
- API docs: http://localhost:8000/docs

Codespaces note: use the **forwarded URL** for ports **3000** and **8000** (not `localhost`).

---

## What DriftQ is doing for you here

In this demo, DriftQ is basically the reliable middle layer between your API and your worker(s) â€” itâ€™s what turns "best-effort background tasks" into **auditable, retryable, replayable runs** with **DLQ** when things go sideways.

Itâ€™s useful because it solves annoying real-world problems like:
- **Retries that donâ€™t suck**: max attempts + backoff, and you can tune it per step (instead of while(true) chaos)
- **DLQ**: failures donâ€™t disappear into logs â€” you get a durable payload you can inspect
- **Replay**: you can re-run the same run after you fix a bug / tweak a prompt / change a toolâ€¦ without inventing your own replay system
- **Durable event stream**: the UI (or logs/observability) can show a real audit trail of what happened and when

The biggest point: you probably donâ€™t want to build workflow logic deep inside your API. It usually starts as "just a couple steps" and turns into a mini orchestration engine:
- ad-hoc retries everywhere
- weird state in the DB
- background jobs that get stuck
- no DLQ, no replay, and impossible debugging

At that point youâ€™ve basically rebuilt a worse version of a workflow engine, but now itâ€™s welded into your API and hard to change. DriftQ is meant to be that "middle layer" so your API stays clean and your workflow behavior stays consistent.

### Why this matters for LLM apps (and not just this toy demo)
LLM workflows are *always* messy:
- tool calls fail, time out, rate-limit, return junk
- you need retries, but you also need to stop retrying eventually
- you need a DLQ payload so you can inspect what actually happened
- you need replay so you can re-run after you tweak prompts/tools/config
- you want an audit trail so you can debug "why did the agent do that?"

DriftQ gives you a clean pattern:
**API emits a command â†’ workers process â†’ events stream back â†’ failures become DLQ â†’ replay when ready.**

And yeah, DriftQâ€‘Core goes way beyond this demo (WAL, partitions, leases, metrics, backpressure, idempotency edge cases, observability, etc). This repo is just the "hello world that feels real".

---

## The 2â€‘Minute Demo (what to click) ğŸ—‚ï¸

1. Open the UI: http://localhost:3000
2. Click **ğŸ—‚ï¸ 2â€‘Minute Demo**
3. Watch the **Demo Script** panel:
   - ğŸŸ¢ step 1: fail (forced)
   - ğŸŸ¢ step 2: DLQ persisted
   - ğŸŸ¢ step 3: replay with fix applied
   - ğŸŸ¢ step 4: success

You can also:
- **View DLQ Payload** to fetch the latest DLQ record
- **Replay Run (fix)** to manually replay with the fix applied
- use **Search JSON** + **type filter**ï¸ in Timeline to find events fast

Codespaces note: if youâ€™re remote, use the **forwarded URL** for port **3000** (not `localhost`).

Example DLQ payload shape:
```json
{
  "type": "runs.dlq",
  "run_id": "...",
  "replay_seq": 0,
  "reason": "max_attempts",
  "error": "forced failure at tool_call",
  "command": { "type": "run.command", "fail_at": "tool_call" }
}
```

---

## Tests âœ…

```bash
cd api
# activate venv
pytest -q
```

---

## Troubleshooting ğŸ”§

### Demo times out waiting for DLQ or success
- manual path: make sure the **worker** is running
- docker path: make sure the containers are up (and not crashing)
- UI should point at API: `NEXT_PUBLIC_API_URL=http://localhost:8000`
  - Codespaces: use the **forwarded URL** for port **8000**

### Replay never reaches Success
Your replay endpoint must accept `{"fail_at": null}` and actually use it.
(Thatâ€™s what makes "Replay (fix applied)" work.)

### DriftQ health endpoint returns "use /v1/* endpoints"
Yep â€” use:
- http://localhost:8080/v1/healthz
